<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title> Software testing</title>

</head>
<body>

<header>
    <article>
        <h1>Soft Engineering</h1>
    </article>
    <article>
        <h1>Chapter 8</h1>
    </article>
        <article>
             <H2> 8.1 Development testing</H2>


        </article>
    
</header>
<nav>
   <a href="">8.1 Development testing</a>
   <a href=""> 8.2 Test-driven development</a>
   <a href=""> 8.3 Release testing</a>
   <a href=""> 8.4 User testing</a>

    
</nav>


        <article>
         <h3> Objectives</h3>
            <p> The objective of this chapter is to introduce software testing and
             software testing processes. When you have read the chapter, you will:</p>


             <ul>

                <li> The objective of this chapter is to introduce software testing and
             software testing processes. When you have read the chapter, you will:</li>
            
                <li> have been introduced to techniques that help you choose test
             cases that are geared to discovering program defects;</li>

                <li>understand test-first development, where you design tests before
             writing code and run these tests automatically;</li>

                <li> know the important differences between component, system,
             and release testing and be aware of user testing processes and
             techniques.</li>

             </ul>


             <h3>Contents</h3>

        <UL>
             <li> 8.1 Development testing</li>
             <li> 8.2 Test-driven development</li>
             <li> 8.3 Release testing</li>
             <li> 8.4 User testing</li>
        </UL>


         <H2>
            8.1 Development testing
        </H2>

             <UL1>

             <p>Testing is intended to show that a program does what it is intended to do and to dis
             cover program defects before it is put into use. When you test software, you execute
             a program using artificial data. You check the results of the test run for errors, anom
             alies, or information about the program’s non-functional attributes.
             The testing process has two distinct goals:</p>


            <p>1. To demonstrate to the developer and the customer that the software meets its
              requirements. For custom software, this means that there should be at least one
             test for every requirement in the requirements document. For generic software
             products, it means that there should be tests for all of the system features, plus
             combinations of these features, that will be incorporated in the product release</p>

             <p> 2. To discover situations in which the behavior of the software is incorrect, unde
             sirable, or does not conform to its specification. These are a consequence of
             software defects. Defect testing is concerned with rooting out undesirable sys
             tem behavior such as system crashes, unwanted interactions with other systems,
             incorrect computations, and data corruption.</p>


            <article>
        </UL1>

            <p> The first goal leads to validation testing, where you expect the system to perform
             correctly using a given set of test cases that reflect the system’s expected use. The
             second goal leads to defect testing, where the test cases are designed to expose
             defects. The test cases in defect testing can be deliberately obscure and need not
             reflect how the system is normally used. Of course, there is no definite boundary
            between these two approaches to testing. During validation testing, you will find
            defects in the system; during defect testing, some of the tests will show that the pro
            gram meets its requirements.</p>


            <p>The diagram shown in Figure 8.1 may help to explain the differences between
         validation testing and defect testing. Think of the system being tested as a black
        box. The system accepts inputs from some input set I and generates outputs in an
        output set O. Some of the outputs will be erroneous. These are the outputs in set Oe
        that are generated by the system in response to inputs in the set Ie. The priority in
        defect testing is to find those inputs in the set Ie because these reveal problems with
        the system. Validation testing involves testing with correct inputs that are outside Ie.
        These stimulate the system to generate the expected correct outputs.</p>

            <p>Testing cannot demonstrate that the software is free of defects or that it will
        behave as specified in every circumstance. It is always possible that a test that you
        have overlooked could discover further problems with the system. As Edsger
        Dijkstra, an early contributor to the development of software engineering, eloquently
        stated (Dijkstra et al., 1972):</p>

            <p> Testing can only show the presence of errors, not their absence</p>

            <p> Testing is part of a broader process of software verification and validation (V & V).
         Verification and validation are not the same thing, although they are often confused</p>

            <img src="Screenshot 2025-09-06 143151.png ">


            <p> Barry Boehm, a pioneer of software engineering, succinctly expressed the difference
        between them (Boehm, 1979):</p>

            <p> Barry Boehm, a pioneer of software engineering, succinctly expressed the difference
         between them (Boehm, 1979):</p>


            <ul>
        
            <li>‘Validation: Are we building the right product?’</li>
            <li>‘Verification: Are we building the product right?’</li>

            </ul>

            </article>

                <p> Verification and validation processes are concerned with checking that software
            being developed meets its specification and delivers the functionality expected by the
            people paying for the software. These checking processes start as soon as requirements
            become available and continue through all stages of the development process.</p>

                 <p>The aim of verification is to check that the software meets its stated functional and
            non-functional requirements. Validation, however, is a more general process. The aim
            of validation is to ensure that the software meets the customer’s expectations. It goes
            beyond simply checking conformance with the specification to demonstrating that the
            software does what the customer expects it to do. Validation is essential because, as 
            I discussed in Chapter 4, requirements specifications do not always reflect the real
            wishes or needs of system customers and users.</p>

                <p> The ultimate goal of verification and validation processes is to establish confi
             dence that the software system is ‘fit for purpose’. This means that the system must
             be good enough for its intended use. The level of required confidence depends on the
             system’s purpose, the expectations of the system users, and the current marketing
             environment for the system:</p>

            <UL>

                <p>1. Software purpose The more critical the software, the more important that it is
             reliable. For example, the level of confidence required for software used to con
             trol a safety-critical system is much higher than that required for a prototype
             that has been developed to demonstrate new product ideas.</p>

             <p> 2. User expectations Because of their experiences with buggy, unreliable software,
             many users have low expectations of software quality. They are not surprised
             when their software fails. When a new system is installed, users may tolerate</p>

                <img src="Screenshot 2025-09-06 150326.png">

             <p><b>Figure 8.2 </b>Inspections and testing</p>

             <p>failures because the benefits of use outweigh the costs of failure recovery.
             In these situations, you may not need to devote as much time to testing the soft
             ware. However, as software matures, users expect it to become more reliable so
             more thorough testing of later versions may be required.</p>

            <p> 3. Marketing environment When a system is marketed, the sellers of the system
             must take into account competing products, the price that customers are willing
             to pay for a system, and the required schedule for delivering that system. In a
             competitive environment, a software company may decide to release a program
             before it has been fully tested and debugged because they want to be the first
             into the market. If a software product is very cheap, users may be willing to tol
            erate a lower level of reliability</p>

            </UL>

             <p> As well as software testing, the verification and validation process may involve
             software inspections and reviews. Inspections and reviews analyze and check the
             system requirements, design models, the program source code, and even proposed
             system tests. These are so-called ‘static’ V & V techniques in which you don’t need
             to execute the software to verify it. Figure 8.2 shows that software inspections and
             testing support V & V at different stages in the software process. The arrows indicate
             the stages in the process where the techniques may be used.</p>

             <p>Inspections mostly focus on the source code of a system but any readable repre
             sentation of the software, such as its requirements or a design model, can be
             inspected. When you inspect a system, you use knowledge of the system, its applica
             tion domain, and the programming or modeling language to discover errors.
             There are three advantages of software inspection over testing:</p>

            <p>1. During testing, errors can mask (hide) other errors. When an error leads to
             unexpected outputs, you can never be sure if later output anomalies are due to
             a new error or are side effects of the original error. Because inspection is a
             static process, you don’t have to be concerned with interactions between
             errors. Consequently, a single inspection session can discover many errors in a system</p>

                  <img src="Screenshot 2025-09-06 151315.png">

             <p> 2. Incomplete versions of a system can be inspected without additional costs. If
             a program is incomplete, then you need to develop specialized test harnesses
             to test the parts that are available. This obviously adds to the system development costs.</p>

                <p> 3. As well as searching for program defects, an inspection can also consider
             broader quality attributes of a program, such as compliance with standards,
             portability, and maintainability. You can look for inefficiencies, inappropriate
             algorithms, and poor programming style that could make the system difficult to
             maintain and update.</p>

             <P> Program inspections are an old idea and there have been several studies and
             experiments that have demonstrated that inspections are more effective for defect
             discovery than program testing. Fagan (1986) reported that more than 60% of the
             errors in a program can be detected using informal program inspections. In the
             Cleanroom process (Prowell et al., 1999), it is claimed that more than 90% of defects
             can be discovered in program inspections.</P>

<P>However, inspections cannot replace software testing. Inspections are not good
for discovering defects that arise because of unexpected interactions between dif
ferent parts of a program, timing problems, or problems with system perfor
mance. Furthermore, especially in small companies or development groups, it can
 be difficult and expensive to put together a separate inspection team as all poten
tial members of the team may also be software developers. I discuss reviews and
 inspections in more detail in Chapter 24 (Quality Management). Automated static
 analysis, where the source text of a program is automatically analyzed to discover
 anomalies, is explained in Chapter 15. In this chapter, I focus on testing and
 testing processes.</P> 

 <P> Figure 8.3 is an abstract model of the ‘traditional’ testing process, as used in plan
driven development. Test cases are specifications of the inputs to the test and the
expected output from the system (the test results), plus a statement of what is being
tested. Test data are the inputs that have been devised to test a system. Test data can
sometimes be generated automatically, but automatic test case generation is impossi
ble, as people who understand what the system is supposed to do must be involved to
specify the expected test results. However, test execution can be automated. The
expected results are automatically compared with the predicted results so there is no
need for a person to look for errors and anomalies in the test run.
</P></P>

   <img src="Screenshot 2025-09-06 151857.png">

   <p><b>Figure 8.3</b> A model of the software testing process Inspections and testing</p>

<P>Typically, a commercial software system has to go through three stages of testing:</P>

<P>
 1. Development testing, where the system is tested during development to discover
 bugs and defects. System designers and programmers are likely to be involved
 in the testing process.
</P>

<P>
 2. Release testing, where a separate testing team tests a complete version of the
 system before it is released to users. The aim of release testing is to check that
 the system meets the requirements of system stakeholders.
</P>

<P> 3. User testing, where users or potential users of a system test the system in their
 own environment. For software products, the ‘user’ may be an internal market
ing group who decide if the software can be marketed, released, and sold.
 Acceptance testing is one type of user testing where the customer formally tests
 a system to decide if it should be accepted from the system supplier or if further
 development is required</P>

 <P>mated testing. In manual testing, a tester runs the program with some test data and
 compares the results to their expectations. They note and report discrepancies to the
 program developers. In automated testing, the tests are encoded in a program that is
 run each time the system under development is to be tested. This is usually faster
 than manual testing, especially when it involves regression testing—re-running pre
 vious tests to check that changes to the program have not introduced new bugs.</P>

 <P> The use of automated testing has increased considerably over the past few years.
 However, testing can never be completely automated as automated tests can only
 check that a program does what it is supposed to do. It is practically impossible to use
 automated testing to test systems that depend on how things look (e.g., a graphical
 user interface), or to test that a program does not have unwanted side effects</P>

    
 <H2>
    8.1 Development testing
 </H2>

 <P> Development testing includes all testing activities that are carried out by the team
 developing the system. The tester of the software is usually the programmer who
 developed that software, although this is not always the case. Some development
 processes use programmer/tester pairs (Cusamano and Selby, 1998) where each</P>

 <img src="Screenshot 2025-09-06 152721.png">

 <p> programmer has an associated tester who develops tests and assists with the testing
 process. For critical systems, a more formal process may be used, with a separate
 testing group within the development team. They are responsible for developing tests
 and maintaining detailed records of test results.</p>

 <p>During development, testing may be carried out at three levels of granularity:</p>

 <p>
 1. Unit testing, where individual program units or object classes are tested. Unit
 testing should focus on testing the functionality of objects or methods.
 </p>

 <p>
 2. Component testing, where several individual units are integrated to create com
posite components. Component testing should focus on testing component interfaces.
 </p>

 <p>3. System testing, where some or all of the components in a system are integrated
 and the system is tested as a whole. System testing should focus on testing com
ponent interactions.</p>

<p>Development testing is primarily a defect testing process, where the aim of testing
 is to discover bugs in the software. It is therefore usually interleaved with debugging—
 the process of locating problems with the code and changing the program to fix these
 problems</p>

 <h2> 8.1.1 Unit testing</h2>

 <p> Unit testing is the process of testing program components, such as methods or object
 classes. Individual functions or methods are the simplest type of component. Your
 tests should be calls to these routines with different input parameters. You can use
 the approaches to test case design discussed in Section 8.1.2, to design the function
 or method tests.</p>

 <p>When you are testing object classes, you should design your tests to provide coverage of all of the features of the object. This means that you should:</p>

 <ul>
    <li> test all operations associated with the object;</li>
    <li> set and check the value of all attributes associated with the object;</li>
    <li> put the object into all possible states. This means that you should simulate all events that cause a state change</li>

 </ul>

 <img src="Screenshot 2025-09-06 153357.png">

 <p> Consider, for example, the weather station object from the example that I discussed
 in Chapter 7. The interface of this object is shown in Figure 8.4. It has a single attribute,
 which is its identifier. This is a constant that is set when the weather station is installed.
 You therefore only need a test that checks if it has been properly set up. You need to
 define test cases for all of the methods associated with the object such as reportWeather,
 reportStatus, etc. Ideally, you should test methods in isolation but, in some cases, some
 test sequences are necessary. For example, to test the method that shuts down the
 weather station instruments (shutdown), you need to have executed the restart method.</p>

 <p> Generalization or inheritance makes object class testing more complicated. You
 can’t simply test an operation in the class where it is defined and assume that it will
 work as expected in the subclasses that inherit the operation. The operation that is
 inherited may make assumptions about other operations and attributes. These may
 not be valid in some subclasses that inherit the operation. You therefore have to test
 the inherited operation in all of the contexts where it is used.</p>

 <p> To test the states of the weather station, you use a state model, such as the one
 shown in Figure 7.8 in the previous chapter. Using this model, you can identify
 sequences of state transitions that have to be tested and define event sequences to
 force these transitions. In principle, you should test every possible state transition
 sequence, although in practice this may be too expensive. Examples of state
 sequences that should be tested in the weather station include:</p>

 <p>Shutdown → Running → Shutdown
 Configuring → Running → Testing → Transmitting → Running
 Running → Collecting → Running → Summarizing → Transmitting → Running</p>

 <p> Whenever possible, you should automate unit testing. In automated unit testing,
 you make use of a test automation framework (such as JUnit) to write and run your
 program tests. Unit testing frameworks provide generic test classes that you extend
 to create specific test cases. They can then run all of the tests that you have imple
mented and report, often through some GUI, on the success or failure of the tests. An
 entire test suite can often be run in a few seconds so it is possible to execute all the
 tests every time you make a change to the program.
 An automated test has three parts</p>

 <p> 1. A setup part, where you initialize the system with the test case, namely the
 inputs and expected outputs</p>

 <p> 2. A call part, where you call the object or method to be tested.</p>

 <p> 3. An assertion part where you compare the result of the call with the expected
 result. If the assertion evaluates to true, the test has been successful; if false,
 then it has failed</p>

 <p>Sometimes the object that you are testing has dependencies on other objects that
 may not have been written or which slow down the testing process if they are used.
 For example, if your object calls a database, this may involve a slow setup process
 before it can be used. In these cases, you may decide to use mock objects. Mock
 objects are objects with the same interface as the external objects being used that
 simulate its functionality. Therefore, a mock object simulating a database may have
 only a few data items that are organized in an array. They can therefore be accessed
 quickly, without the overheads of calling a database and accessing disks. Similarly,
 mock objects can be used to simulate abnormal operation or rare events. For exam
ple, if your system is intended to take action at certain times of day, your mock
 object can simply return those times, irrespective of the actual clock time</p>


     <h2> 8.1.2 Choosing unit test cases</h2>

 <p>Testing is expensive and time consuming, so it is important that you choose effective
 unit test cases. Effectiveness, in this case, means two things:</p>

 <p>1. The test cases should show that, when used as expected, the component that you
 are testing does what it is supposed to do.</p>

  <p> 2. If there are defects in the component, these should be revealed by test cases.</p>

  <p> You should therefore write two kinds of test case. The first of these should reflect
 normal operation of a program and should show that the component works. For
 example, if you are testing a component that creates and initializes a new patient
 record, then your test case should show that the record exists in a database and that
 its fields have been set as specified. The other kind of test case should be based on
 testing experience of where common problems arise. It should use abnormal inputs
 to check that these are properly processed and do not crash the component.</p>

 <p>I discuss two possible strategies here that can be effective in helping you choose
 test cases. These are:</p>

 <p>  1. Partition testing, where you identify groups of inputs that have common charac
teristics and should be processed in the same way. You should choose tests from within each of these groups.</p>


<p> 2. Guideline-based testing, where you use testing guidelines to choose test cases.
 These guidelines reflect previous experience of the kinds of errors that program
mers often make when developing components
</p>


    <img src="Screenshot 2025-09-06 153357.png">

<p> The input data and output results of a program often fall into a number of differ
ent classes with common characteristics. Examples of these classes are positive
 numbers, negative numbers, and menu selections. Programs normally behave in a
 comparable way for all members of a class. That is, if you test a program that does a
 computation and requires two positive numbers, then you would expect the program
 to behave in the same way for all positive numbers.</p>

 <p> Because of this equivalent behavior, these classes are sometimes called equiva
lence partitions or domains (Bezier, 1990). One systematic approach to test case
 design is based on identifying all input and output partitions for a system or compo
nent. Test cases are designed so that the inputs or outputs lie within these partitions.
 Partition testing can be used to design test cases for both systems and components.</p>

 <p> In Figure 8.5, the large shaded ellipse on the left represents the set of all possible
 inputs to the program that is being tested. The smaller unshaded ellipses represent
 equivalence partitions. A program being tested should process all of the members of
 an input equivalence partitions in the same way. Output equivalence partitions are
 partitions within which all of the outputs have something in common. Sometimes
 there is a 1:1 mapping between input and output equivalence partitions. However,
 this is not always the case; you may need to define a separate input equivalence par
tition, where the only common characteristic of the inputs is that they generate out
puts within the same output partition. The shaded area in the left ellipse represents
 inputs that are invalid. The shaded area in the right ellipse represents exceptions that
 may occur (i.e., responses to invalid inputs).</p>

 <p>Once you have identified a set of partitions, you choose test cases from each of
 these partitions. A good rule of thumb for test case selection is to choose test cases
 on the boundaries of the partitions, plus cases close to the midpoint of the partition.
 The reason for this is that designers and programmers tend to consider typical values
 of inputs when developing a system. You test these by choosing the midpoint of the
 partition. Boundary values are often atypical (e.g., zero may behave differently from
 other non-negative numbers) so are sometimes overlooked by developers. Program
 failures often occur when processing these atypical values</p>


 <img src="Screenshot 2025-09-06 154447.png">

 <p> You identify partitions by using the program specification or user documentation
 and from experience where you predict the classes of input value that are likely to
 detect errors. For example, say a program specification states that the program
 accepts 4 to 8 inputs which are five-digit integers greater than 10,000. You use this
 information to identify the input partitions and possible test input values. These are
 shown in Figure 8.6.</p>

 <p>When you use the specification of a system to identify equivalence partitions, this
 is called ‘black-box testing’. Here, you don’t need any knowledge of how the system
 works. However, it may be helpful to supplement the black-box tests with ‘white
box testing’, where you look at the code of the program to find other possible tests.
 For example, your code may include exceptions to handle incorrect inputs. You can
 use this knowledge to identify ‘exception partitions’—different ranges where the
 same exception handling should be applied.</p>

 <p> Equivalence partitioning is an effective approach to testing because it helps
 account for errors that programmers often make when processing inputs at the edges
 of partitions. You can also use testing guidelines to help choose test cases.
 Guidelines encapsulate knowledge of what kinds of test cases are effective for dis
covering errors. For example, when you are testing programs with sequences, arrays,
 or lists, guidelines that could help reveal defects include:</p>

<p>  1. Test software with sequences that have only a single value. Programmers natu
rally think of sequences as made up of several values and sometimes they embed
 this assumption in their programs. Consequently, if presented with a single
value sequence, a program may not work properly.</p>

<p> 2. Use different sequences of different sizes in different tests. This decreases the
 chances that a program with defects will accidentally produce a correct output
 because of some accidental characteristics of the input.</p>

<p> 3. Derive tests so that the first, middle, and last elements of the sequence are
 accessed. This approach is reveals problems at partition boundaries
</p>

 <img src="Screenshot 2025-09-06 154750.png">

 <p> Whittaker’s book (2002) includes many examples of guidelines that can be used
 in test case design. Some of the most general guidelines that he suggests are:</p>

 <ul>
    <li>Choose inputs that force the system to generate all error messages;</li>
    <li>Design inputs that cause input buffers to overflow;</li>
    <li>Repeat the same input or series of inputs numerous times;</li>
    <li> Force invalid outputs to be generated</li>
    <li>Force computation results to be too large or too small.</li>
    
 </ul>

 <p>As you gain experience with testing, you can develop your own guidelines about
 how to choose effective test cases. I give more examples of testing guidelines in the
 next section of this chapter.</p>

 



    


     <h2> 8.1.3 Component testing</h2>

    <p>Software components are often composite components that are made up of several
     interacting objects. For example, in the weather station system, the reconfiguration
 component includes objects that deal with each aspect of the reconfiguration. You
 access the functionality of these objects through the defined component interface.
 Testing composite components should therefore focus on showing that the compo
nent interface behaves according to its specification. You can assume that unit tests
 on the individual objects within the component have been completed.</p>

 <p> Figure 8.7 illustrates the idea of component interface testing. Assume that compo
nents A, B, and C have been integrated to create a larger component or subsystem.
 The test cases are not applied to the individual components but rather to the interface
 of the composite component created by combining these components. Interface errors
 in the composite component may not be detectable by testing the individual objects
 because these errors result from interactions between the objects in the component.
</p>

<p> There are different types of interface between program components and, conse
quently, different types of interface error that can occur:
</p>

<p> 1. Parameter interfaces These are interfaces in which data or sometimes function
 references are passed from one component to another. Methods in an object
 have a parameter interface.
</p>

<img src="Screenshot 2025-09-06 155443.png">

<p> 2. Shared memory interfaces These are interfaces in which a block of memory is
 shared between components. Data is placed in the memory by one subsystem
 and retrieved from there by other sub-systems. This type of interface is often
 used in embedded systems, where sensors create data that is retrieved and
 processed by other system components.</p>

<p> 3. Procedural interfaces These are interfaces in which one component encapsu
lates a set of procedures that can be called by other components. Objects and
 reusable components have this form of interface.</p>

<p> 4. Message passing interfaces These are interfaces in which one component
 requests a service from another component by passing a message to it. A return
 message includes the results of executing the service. Some object-oriented sys
tems have this form of interface, as do client–server systems</p>


<p> Interface errors are one of the most common forms of error in complex systems
 (Lutz, 1993). These errors fall into three classes:</p>

 <ul>
    <li> Interface misuse A calling component calls some other component and makes an
 error in the use of its interface. This type of error is common with parameter inter
faces, where parameters may be of the wrong type or be passed in the wrong
 order, or the wrong number of parameters may be passed</li>
    <li> Interface misunderstanding A calling component misunderstands the specifica
tion of the interface of the called component and makes assumptions about its
 behavior. The called component does not behave as expected which then causes
 unexpected behavior in the calling component. For example, a binary search
 method may be called with a parameter that is an unordered array. The search
 would then fail.</li>
    <li>Timing errors These occur in real-time systems that use a shared memory or a
 message-passing interface. The producer of data and the consumer of data may
 operate at different speeds. Unless particular care is taken in the interface design,
 the consumer can access out-of-date information because the producer of the
 information has not updated the shared interface information</li>

 </ul>

<p>Testing for interface defects is difficult because some interface faults may only
 manifest themselves under unusual conditions. For example, say an object imple
ments a queue as a fixed-length data structure. A calling object may assume that the
 queue is implemented as an infinite data structure and may not check for queue over
flow when an item is entered. This condition can only be detected during testing by
 designing test cases that force the queue to overflow and cause that overflow to cor
rupt the object behavior in some detectable way.</p>

<p> A further problem may arise because of interactions between faults in different
 modules or objects. Faults in one object may only be detected when some other object
 behaves in an unexpected way. For example, an object may call another object to
 receive some service and assume that the response is correct. If the called service is
 faulty in some way, the returned value may be valid but incorrect. This is not immedi
ately detected but only becomes obvious when some later computation goes wrong.
 Some general guidelines for interface testing are:</p>

 <p> 1. Examine the code to be tested and explicitly list each call to an external compo
nent. Design a set of tests in which the values of the parameters to the external
 components are at the extreme ends of their ranges. These extreme values are
 most likely to reveal interface inconsistencies.</p>

<p> 2. Where pointers are passed across an interface, always test the interface with null
 pointer parameters.</p>

<p> 3. Where a component is called through a procedural interface, design tests that
 deliberately cause the component to fail. Differing failure assumptions are one
 of the most common specification misunderstandings.</p>

 <p> 4. Use stress testing in message passing systems. This means that you should
 design tests that generate many more messages than are likely to occur in prac
tice. This is an effective way of revealing timing problems.</p>

\<p> 5. Where several components interact through shared memory, design tests that
 vary the order in which these components are activated. These tests may reveal
 implicit assumptions made by the programmer about the order in which the
 shared data is produced and consumed</p>

 <p> Inspections and reviews can sometimes be more cost effective than testing for
 discovering interface errors. Inspections can concentrate on component interfaces
 and questions about the assumed interface behavior asked during the inspection
 process. A strongly typed language such as Java allows many interface errors to be
 trapped by the compiler. Static analyzers (see Chapter 15) can detect a wide range
 of interface errors</p>

 <h2> 8.1.4 System testing</h2>

 <p> System testing during development involves integrating components to create a ver
sion of the system and then testing the integrated system. System testing checks that
 components are compatible, interact correctly and transfer the right data at the right
 time across their interfaces. It obviously overlaps with component testing but there
 are two important differences:</p>

 <p> 1. During system testing, reusable components that have been separately devel
oped and off-the-shelf systems may be integrated with newly developed compo
nents. The complete system is then tested.</p>

<p> 2. Components developed by different team members or groups may be integrated
 at this stage. System testing is a collective rather than an individual process. In
 some companies, system testing may involve a separate testing team with no
 involvement from designers and programmers.</p>

 <p> When you integrate components to create a system, you get emergent behavior.
 This means that some elements of system functionality only become obvious when
 you put the components together. This may be planned emergent behavior, which
 has to be tested. For example, you may integrate an authentication component with a
 component that updates information. You then have a system feature that restricts
 information updating to authorized users. Sometimes, however, the emergent
 behavior is unplanned and unwanted. You have to develop tests that check that the
 system is only doing what it is supposed to do.</p>

 <p> Therefore system testing should focus on testing the interactions between the
 components and objects that make up a system. You may also test reusable compo
nents or systems to check that they work as expected when they are integrated with
 new components. This interaction testing should discover those component bugs that
 are only revealed when a component is used by other components in the system.
 Interaction testing also helps find misunderstandings, made by component develop
ers, about other components in the system.</p>

<p> Because of its focus on interactions, use case–based testing is an effective
 approach to system testing. Typically, each use case is implemented by several com
ponents or objects in the system. Testing the use case forces these interactions to occur. 
If you have developed a sequence diagram to model the use case implementa
tion, you can see the objects or components that are involved in the interaction.</p>

<img src="Screenshot 2025-09-06 160539.png">

<p><b> Figure 8.8</b>  Collectweather data sequence chart</p>


<p>To illustrate this, I use an example from the wilderness weather station system
 where the weather station is asked to report summarized weather data to a remote
 computer. The use case for this is described in Figure 7.3 (see previous chapter).
 Figure 8.8 (which is a copy of Figure 7.7) shows the sequence of operations in the
 weather station when it responds to a request to collect data for the mapping system.
 You can use this diagram to identify operations that will be tested and to help design
 the test cases to execute the tests. Therefore, issuing a request for a report will result
 in the execution of the following thread of methods:</p>

 <p> SatComms:request → WeatherStation:reportWeather → Commslink:Get(summary)
 →WeatherData:summarize</p>

                 <p> The sequence diagram helps you design the specific test cases that you need as it
             shows what inputs are required and what outputs are created:</p>

                <p> 1. An input of a request for a report should have an associated acknowledgment.
             A report should ultimately be returned from the request. During testing, you
             should create summarized data that can be used to check that the report is
             correctly organized.</p>

                 <p> 2. An input request for a report to WeatherStation results in a summarized report
             being generated. You can test this in isolation by creating raw data corre
            sponding to the summary that you have prepared for the test of SatComms and
             checking that the WeatherStation object correctly produces this summary. This
             raw data is also used to test the WeatherData object. Of course, I have simplified the sequence diagram in Figure 8.8 so that it does not
             show exceptions. A complete use case/scenario test must also take these into account
             and ensure that objects correctly handle exceptions.</p>


                <p> For most systems, it is difficult to know how much system testing is essential and
             when you should to stop testing. Exhaustive testing, where every possible program
             execution sequence is tested, is impossible. Testing, therefore, has to be based on a
             subset of possible test cases. Ideally, software companies should have policies for
             choosing this subset. These policies might be based on general testing policies, such
             as a policy that all program statements should be executed at least once.
             Alternatively, they may be based on experience of system usage and focus on testing
             the features of the operational system. For example:</p>

             <p>1. All system functions that are accessed through menus should be tested.</p>

            <p> 2. Combinations of functions (e.g., text formatting) that are accessed through the
             same menu must be tested.</p>

             <p> 3. Where user input is provided, all functions must be tested with both correct and
             incorrect input</p>

             <p> It is clear from experience with major software products such as word processors
         or spreadsheets that similar guidelines are normally used during product testing.
         When features of the software are used in isolation, they normally work. Problems
         arise, as Whittaker (2002) explains, when combinations of less commonly used fea
         tures have not been tested together. He gives the example of how, in a commonly
         used word processor, using footnotes with a multicolumn layout causes incorrect
         layout of the text.</p>

         <p>Automated system testing is usually more difficult than automated unit or compo
          nent testing. Automated unit testing relies on predicting the outputs then encoding
          these predictions in a program. The prediction is then compared with the result.
          However, the point of implementing a system may be to generate outputs that are
          large or cannot be easily predicted. You may be able to examine an output and check
          its credibility without necessarily being able to create it in advance.</p>


    <article>

         <h2> 8.2 Test-driven development</h2>

 <p> Test-driven development (TDD) is an approach to program development in which
 you interleave testing and code development (Beck, 2002; Jeffries and Melnik,
 2007). Essentially, you develop the code incrementally, along with a test for that
 increment. You don’t move on to the next increment until the code that you have
 developed passes its test. Test-driven development was introduced as part of agile
 methods such as Extreme Programming. However, it can also be used in plan-driven
 development processes</p>

     <img src="Screenshot 2025-09-06 161408.png">

<p><b>  Figure 8.9 </b>  Test-driven developmen</p>

<p>The fundamental TDD process is shown in Figure 8.9. The steps in the process
 are as follows:</p>

 <p> 1. You start by identifying the increment of functionality that is required. This
 should normally be small and implementable in a few lines of code</p>

 <p> 2. You write a test for this functionality and implement this as an automated test.
 This means that the test can be executed and will report whether or not it has
 passed or failed.</p>

 <p>3. You then run the test, along with all other tests that have been implemented.
 Initially, you have not implemented the functionality so the new test will fail.
 This is deliberate as it shows that the test adds something to the test set.</p>

 <p> 4. You then implement the functionality and re-run the test. This may involve
 refactoring existing code to improve it and add new code to what’s already there.</p>

 <p> 5. Once all tests run successfully, you move on to implementing the next chunk of
 functionality</p>

 <p> An automated testing environment, such as the JUnit environment that supports
 Java program testing (Massol and Husted, 2003), is essential for TDD. As the code
 is developed in very small increments, you have to be able to run every test each time
 that you add functionality or refactor the program. Therefore, the tests are embedded
 in a separate program that runs the tests and invokes the system that is being tested.
 Using this approach, it is possible to run hundreds of separate tests in a few seconds.</p>

 <p> A strong argument for test-driven development is that it helps programmers clarify
 their ideas of what a code segment is actually supposed to do. To write a test, you need
 to understand what is intended, as this understanding makes it easier to write the
 required code. Of course, if you have incomplete knowledge or understanding, then
 test-driven development won’t help. If you don’t know enough to write the tests, you
 won’t develop the required code. For example, if your computation involves division,
 you should check that you are not dividing the numbers by zero. If you forget to write
 a test for this, then the code to check will never be included in the program.</p>

 <p> As well as better problem understanding, other benefits of test-driven development are:</p>

 <p> 1. Code coverage In principle, every code segment that you write should have at
 least one associated test. Therefore, you can be confident that all of the code in  the system has actually been executed. Code is tested as it is written so defects
 are discovered early in the development process.</p>

 <p>2. Regression testing A test suite is developed incrementally as a program is devel
oped. You can always run regression tests to check that changes to the program
 have not introduced new bugs.</p>

 <p> 3. Simplified debugging When a test fails, it should be obvious where the problem
 lies. The newly written code needs to be checked and modified. You do not need
 to use debugging tools to locate the problem. Reports of the use of test-driven
 development suggest that it is hardly ever necessary to use an automated debug
ger in test-driven development (Martin, 2007).</p>

<p> 4. System documentation The tests themselves act as a form of documentation that
 describe what the code should be doing. Reading the tests can make it easier to
 understand the code</p>

 <p> One of the most important benefits of test-driven development is that it reduces
 the costs of regression testing. Regression testing involves running test sets that have
 successfully executed after changes have been made to a system. The regression test
 checks that these changes have not introduced new bugs into the system and that the
 new code interacts as expected with the existing code. Regression testing is very
 expensive and often impractical when a system is manually tested, as the costs in
 time and effort are very high. In such situations, you have to try and choose the most
 relevant tests to re-run and it is easy to miss important tests.</p>

 <p> However, automated testing, which is fundamental to test-first development, dra
matically reduces the costs of regression testing. Existing tests may be re-run
 quickly and cheaply. After making a change to a system in test-first development, all
 existing tests must run successfully before any further functionality is added. As a
 programmer, you can be confident that the new functionality that you have added has
 not caused or revealed problems with existing code.</p>

 <p>Test-driven development is of most use in new software development where the
 functionality is either implemented in new code or by using well-tested standard
 libraries. If you are reusing large code components or legacy systems then you need
 to write tests for these systems as a whole. Test-driven development may also be
 ineffective with multi-threaded systems. The different threads may be interleaved at
 different times in different test runs, and so may produce different results.</p>


        <P> If you use test-driven development, you still need a system testing process to val
     idate the system; that is, to check that it meets the requirements of all of the system
     stakeholders. System testing also tests performance, reliability, and checks that the
     system does not do things that it shouldn’t do, such as produce unwanted outputs,
     etc. Andrea (2007) suggests how testing tools can be extended to integrate some
     aspects of system testing with TDD.</P>

        <P> Test-driven development has proved to be a successful approach for small and
     medium-sized projects. Generally, programmers who have adopted this approach are
     happy with it and find it a more productive way to develop software (Jeffries and Melnik, 2007). In some trials, it has been shown to lead to improved code quality; in
     others, the results have been inconclusive. However, there is no evidence that TDD
     leads to poorer quality code</P>

     </article>


     <article>


        <h2> 8.3 Release testing</h2>

        parag








     </article>



   



























































    </article>
</body>
</html>


   
       